{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from simplejson import loads\n",
    "from snomedct_terminology_server.server.models import Description\n",
    "from snomedct_terminology_server.server.utils import execute_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# word_stats_query = \"\"\"\n",
    "# SELECT json_object(array_agg(word::text),array_agg(nentry::text)) \n",
    "# FROM ts_stat('SELECT to_tsvector(''simple'', term) \n",
    "#     FROM denormalized_description_for_current_snapshot')\"\"\"\n",
    "# word_stats_query = \"\"\"SELECT * from description_terms\"\"\"\n",
    "# results=execute_query(word_stats_query)\n",
    "with open('description_term_counts.json') as f:\n",
    "    results = loads(f.read())\n",
    "expr_1 = '(\\w+[-/]\\w+|\\w+)+'\n",
    "expr_2 = '((\\w+[-/])+\\w+|\\w+)+'\n",
    "expr_3 = '((\\w+[-/])+\\w+|\\w+[-/]\\w+|\\w+)+'\n",
    "expr_4 = '((\\w+[-/.])+\\w+|\\w+[-/]\\w+|\\w+|(\\w/\\w+)+|^/\\w+|(^/\\w+[-/])+\\w+|(^/\\w+[-/]\\w+[-]\\w+)+)+'\n",
    "def words(text): return re.findall(expr_4, text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "WORDS = {}\n",
    "for word, nentry in results.items():\n",
    "    try:\n",
    "        key = words(word)[0]\n",
    "#         print({key[0]: word})\n",
    "        WORDS.update({key[0]: int(nentry)})\n",
    "    except IndexError:\n",
    "        print(\"found no text match for: {}\".format(word))\n",
    "        \n",
    "print(len(results.keys()))\n",
    "print(len(WORDS.keys()))\n",
    "print(len(set(results.keys()).difference(WORDS.keys())))\n",
    "eliminated_results = set(results.keys()).difference(WORDS.keys())\n",
    "print('/recombinant-b-subunit' in eliminated_results)\n",
    "print({key: results[key] for key in eliminated_results})\n",
    "print(WORDS['vapour-permeable'])\n",
    "\n",
    "print(WORDS['recombinant-b-subunit'])\n",
    "\n",
    "print(WORDS['frequency-doubled'])\n",
    "# print(WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def words(text): return re.findall(r'\\w+', text.lower())\n",
    "# WORDS = Counter(words(text))\n",
    "\n",
    "def P(word, N=sum(WORDS.values())):\n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word):\n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word):\n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words):\n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word):\n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################ Test Code\n",
    "\n",
    "def unit_tests():\n",
    "    print(correction('peling'))\n",
    "    assert correction('peling') == 'peeling'                  # insert\n",
    "#     assert correction('korrectud') == 'corrected'           # replace 2\n",
    "#     assert correction('bycycle') == 'bicycle'               # replace\n",
    "#     assert correction('inconvient') == 'inconvenient'       # insert 2\n",
    "#     assert correction('arrainged') == 'arranged'            # delete\n",
    "    assert correction('vapour-preeable') == 'vapour-permeable'              # transpose + insert\n",
    "#     assert correction('premeable') == 'permeable'             # transpose\n",
    "    assert correction('clinicla') == 'clinical'               # transpose\n",
    "    assert correction('cliniclaa') =='clinical'               # transpose + delete\n",
    "    assert correction('wsa') ==  'was'\n",
    "#     assert correction('word') == 'word'                     # known\n",
    "#     assert correction('quintessential') == 'quintessential' # unknown\n",
    "#     assert words('This is a TEST.') == ['this', 'is', 'a', 'test']\n",
    "#     assert Counter(words('This is a test. 123; A TEST this is.')) == (\n",
    "#            Counter({'123': 1, 'a': 2, 'is': 2, 'test': 2, 'this': 2}))\n",
    "#     assert len(WORDS) == 32192\n",
    "#     assert sum(WORDS.values()) == 1115504\n",
    "#     assert WORDS.most_common(10) == [\n",
    "#      ('the', 79808),\n",
    "#      ('of', 40024),\n",
    "#      ('and', 38311),\n",
    "#      ('to', 28765),\n",
    "#      ('in', 22020),\n",
    "#      ('a', 21124),\n",
    "#      ('that', 12512),\n",
    "#      ('he', 12401),\n",
    "#      ('was', 11410),\n",
    "#      ('it', 10681)]\n",
    "#     assert WORDS['the'] == 79808\n",
    "#     assert P('quintessential') == 0\n",
    "#     assert 0.07 < P('the') < 0.08\n",
    "    return 'unit_tests pass'\n",
    "\n",
    "def spelltest(tests, verbose=False):\n",
    "    \"Run correction(wrong) on all (right, wrong) pairs; report results.\"\n",
    "    import time\n",
    "    start = time.clock()\n",
    "    good, unknown = 0, 0\n",
    "    n = len(tests)\n",
    "    for right, wrong in tests:\n",
    "        w = correction(wrong)\n",
    "        good += (w == right)\n",
    "        if w != right:\n",
    "            unknown += (right not in WORDS)\n",
    "            if verbose:\n",
    "                print('correction({}) => {} ({}); expected {} ({})'\n",
    "                      .format(wrong, w, WORDS[w], right, WORDS[right]))\n",
    "    dt = time.clock() - start\n",
    "    print('{:.0%} of {} correct ({:.0%} unknown) at {:.0f} words per second '\n",
    "          .format(good / n, n, unknown / n, n / dt))\n",
    "\n",
    "def Testset(lines):\n",
    "    \"Parse 'right: wrong1 wrong2' lines into [('right', 'wrong1'), ('right', 'wrong2')] pairs.\"\n",
    "    return [(right, wrong)\n",
    "            for (right, wrongs) in (line.split(':') for line in lines)\n",
    "            for wrong in wrongs.split()]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(unit_tests())\n",
    "#     spelltest(Testset(open('spell-testset1.txt')))\n",
    "#     spelltest(Testset(open('spell-testset2.txt')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
